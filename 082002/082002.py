import urllib.parse
import requests
import re
import time
import json
import os

def save_to_master_json(new_data, keyword, filename):
    """
    å°‡æ–°çš„æœå°‹çµæœç´¯ç©åˆ°ä¸»è¦ JSON æª”æ¡ˆä¸­ï¼Œä¸¦å»é‡
    """
    try:
        # å˜—è©¦è®€å–ç¾æœ‰çš„æª”æ¡ˆ
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                master_data = json.load(f)
        else:
            # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå‰µå»ºæ–°çš„çµæ§‹
            master_data = {
                "created_time": time.strftime('%Y-%m-%d %H:%M:%S'),
                "last_updated": "",
                "search_history": [],
                "total_unique_links": 0,
                "all_links": []
            }
        
        # æ›´æ–°æœ€å¾Œä¿®æ”¹æ™‚é–“
        master_data["last_updated"] = time.strftime('%Y-%m-%d %H:%M:%S')
        
        # è¨˜éŒ„é€™æ¬¡æœå°‹
        search_record = {
            "keyword": keyword,
            "search_time": time.strftime('%Y-%m-%d %H:%M:%S'),
            "new_links_found": len(new_data)
        }
        master_data["search_history"].append(search_record)
        
        # å–å¾—ç¾æœ‰é€£çµçš„ URL é›†åˆï¼ˆç”¨æ–¼å»é‡ï¼‰
        existing_urls = {link["url"] for link in master_data["all_links"]}
        
        # æ·»åŠ æ–°é€£çµï¼ˆå»é‡ï¼‰
        new_links_added = 0
        for link in new_data:
            if link["url"] not in existing_urls:
                # æ·»åŠ æœå°‹é—œéµå­—è³‡è¨Š
                link_with_keyword = link.copy()
                link_with_keyword["found_by_keyword"] = keyword
                link_with_keyword["found_time"] = time.strftime('%Y-%m-%d %H:%M:%S')
                
                master_data["all_links"].append(link_with_keyword)
                existing_urls.add(link["url"])
                new_links_added += 1
        
        # æ›´æ–°çµ±è¨ˆ
        master_data["total_unique_links"] = len(master_data["all_links"])
        search_record["new_unique_links_added"] = new_links_added
        
        # ä¿å­˜æª”æ¡ˆ
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(master_data, f, ensure_ascii=False, indent=2)
        
        print(f"  æœ¬æ¬¡æ–°å¢ {new_links_added} å€‹ä¸é‡è¤‡é€£çµ")
        print(f"  ç´¯ç©ç¸½é€£çµæ•¸ï¼š{master_data['total_unique_links']}")
        
    except Exception as e:
        print(f"ä¿å­˜åˆ°ä¸»æª”æ¡ˆéŒ¯èª¤: {e}")


def read_keywords_from_file(filename):
    """
    å¾æ–‡å­—æª”æ¡ˆä¸­è®€å–é—œéµå­—åˆ—è¡¨
    æ¯è¡Œä¸€å€‹é—œéµå­—ï¼Œå¿½ç•¥ç©ºè¡Œå’Œä»¥#é–‹é ­çš„è¨»è§£è¡Œ
    """
    keywords = []
    try:
        if not os.path.exists(filename):
            print(f"é—œéµå­—æª”æ¡ˆ {filename} ä¸å­˜åœ¨")
            return keywords
            
        with open(filename, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                # å¿½ç•¥ç©ºè¡Œå’Œè¨»è§£è¡Œ
                if line and not line.startswith('#'):
                    keywords.append(line)
                    print(f"è®€å–é—œéµå­— {line_num}: {line}")
        
        print(f"æˆåŠŸè®€å– {len(keywords)} å€‹é—œéµå­—")
        return keywords
        
    except Exception as e:
        print(f"è®€å–é—œéµå­—æª”æ¡ˆéŒ¯èª¤: {e}")
        return keywords

def batch_search_from_file(keywords_file, max_results=10, delay_between_searches=2):
    """
    å¾æª”æ¡ˆè®€å–é—œéµå­—ä¸¦é€²è¡Œæ‰¹é‡æœç´¢
    """
    keywords = read_keywords_from_file(keywords_file)
    
    if not keywords:
        print("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é—œéµå­—")
        return
    
    print(f"\né–‹å§‹æ‰¹é‡æœç´¢ï¼Œå…± {len(keywords)} å€‹é—œéµå­—")
    print(f"æ¯æ¬¡æœç´¢é–“éš” {delay_between_searches} ç§’")
    print("="*60)
    
    total_results = 0
    successful_searches = 0
    
    for i, keyword in enumerate(keywords, 1):
        print(f"\n[{i}/{len(keywords)}] æœç´¢é—œéµå­—: {keyword}")
        print("-" * 40)
        
        try:
            # åŸ·è¡Œæœç´¢
            results = search_via_multiple_engines(keyword, max_results)
            
            if results:
                successful_searches += 1
                total_results += len(results)
                
                # ä¿å­˜å€‹åˆ¥æœç´¢çµæœ
                save_individual_results(results, keyword)
                
                # ç´¯ç©åˆ°ä¸»æª”æ¡ˆ
                master_json_filename = "ptt_links_master.json"
                save_to_master_json(results, keyword, master_json_filename)
                
                print(f"âœ“ æ‰¾åˆ° {len(results)} å€‹çµæœ")
            else:
                print("âœ— æ²’æœ‰æ‰¾åˆ°çµæœ")
            
            # æœç´¢é–“éš”ï¼ˆæœ€å¾Œä¸€æ¬¡æœç´¢ä¸éœ€è¦ç­‰å¾…ï¼‰
            if i < len(keywords):
                print(f"ç­‰å¾… {delay_between_searches} ç§’å¾Œç¹¼çºŒ...")
                time.sleep(delay_between_searches)
                
        except Exception as e:
            print(f"âœ— æœç´¢å¤±æ•—: {e}")
    
    # é¡¯ç¤ºæ‰¹é‡æœç´¢ç¸½çµ
    print("\n" + "="*60)
    print("æ‰¹é‡æœç´¢å®Œæˆï¼")
    print(f"æˆåŠŸæœç´¢: {successful_searches}/{len(keywords)} å€‹é—œéµå­—")
    print(f"ç¸½å…±æ‰¾åˆ°: {total_results} å€‹çµæœ")
    print(f"çµæœå·²ç´¯ç©ä¿å­˜åˆ°: ptt_links_master.json")
    print("="*60)

def save_individual_results(results, keyword):
    """
    ä¿å­˜å€‹åˆ¥é—œéµå­—çš„æœç´¢çµæœ
    """
    if not results:
        return
        
    # ä¿å­˜è©³ç´°çµæœåˆ°æ–‡å­—æª”
    txt_filename = f"ptt_search_{keyword.replace(' ', '_').replace('/', '_')}.txt"
    with open(txt_filename, 'w', encoding='utf-8') as f:
        f.write(f"PTT æœå°‹çµæœ\n")
        f.write(f"é—œéµå­—ï¼š{keyword}\n")
        f.write(f"æœå°‹æ™‚é–“ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*50 + "\n\n")
        
        for i, result in enumerate(results, 1):
            f.write(f"{i}. {result['title']}\n")
            f.write(f"   é€£çµï¼š{result['url']}\n")
            f.write(f"   ä¾†æºï¼š{result['source']}\n\n")
    
    # ä¿å­˜ç´”é€£çµåˆ—è¡¨
    links_filename = f"ptt_urls_{keyword.replace(' ', '_').replace('/', '_')}.txt"
    with open(links_filename, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(f"{result['url']}\n")

def search_via_multiple_engines(query, max_results=10):
    """
    ä½¿ç”¨å¤šå€‹æœç´¢å¼•æ“æœå°‹ PTT ç›¸é—œçµæœ
    ç•¶ä¸€å€‹å¼•æ“è¢«å°é–æ™‚è‡ªå‹•åˆ‡æ›åˆ°å…¶ä»–å¼•æ“
    """
    results = []
    
    # é™åˆ¶æœ€å¤§çµæœæ•¸
    if max_results > 20:
        print(f"âš ï¸  å°‡æœ€å¤§çµæœæ•¸å¾ {max_results} èª¿æ•´ç‚º 20ï¼ˆé¿å…è¢«å°é–ï¼‰")
        max_results = 20
    
    print(f"ğŸ” æœå°‹é—œéµå­—: {query} (æœ€å¤š {max_results} ç­†)")
    
    # å˜—è©¦å¤šå€‹æœç´¢å¼•æ“
    search_engines = [
        ("DuckDuckGo", search_via_duckduckgo),
        ("Bing", search_via_bing),
        ("Startpage", search_via_startpage_original)
    ]
    
    for engine_name, search_func in search_engines:
        print(f"\nğŸ”„ å˜—è©¦ä½¿ç”¨ {engine_name} æœç´¢...")
        try:
            engine_results = search_func(query, max_results)
            if engine_results:
                results.extend(engine_results)
                print(f"âœ… {engine_name} æ‰¾åˆ° {len(engine_results)} å€‹çµæœ")
                break  # æ‰¾åˆ°çµæœå°±åœæ­¢å˜—è©¦å…¶ä»–å¼•æ“
            else:
                print(f"âš ï¸  {engine_name} æ²’æœ‰æ‰¾åˆ°çµæœ")
        except Exception as e:
            print(f"âŒ {engine_name} æœç´¢å¤±æ•—: {e}")
    
    # å»é‡ä¸¦é™åˆ¶çµæœæ•¸é‡
    unique_results = []
    seen_urls = set()
    
    for result in results:
        if result['url'] not in seen_urls and len(unique_results) < max_results:
            unique_results.append(result)
            seen_urls.add(result['url'])
    
    print(f"ğŸ¯ å®Œæˆæœå°‹ï¼Œå…±æ‰¾åˆ° {len(unique_results)} å€‹ä¸é‡è¤‡çµæœ")
    return unique_results

def search_via_duckduckgo(query, max_results=10):
    """ä½¿ç”¨ DuckDuckGo æœç´¢"""
    results = []
    
    try:
        search_query = f"{query} site:ptt.cc"
        encoded_query = urllib.parse.quote(search_query)
        
        url = f"https://duckduckgo.com/html/?q={encoded_query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        }
        
        session = requests.Session()
        session.trust_env = False
        
        response = session.get(url, headers=headers, timeout=15, proxies={})
        
        if response.status_code == 200:
            content = response.text
            
            # æ‰¾å‡ºPTTé€£çµ
            ptt_pattern = r'https?://(?:www\.)?ptt\.cc/bbs/[^/]+/M\.\d+\.A\.[A-Z0-9]+\.html'
            ptt_links = list(set(re.findall(ptt_pattern, content)))
            
            for i, link in enumerate(ptt_links[:max_results]):
                # æå–çœ‹æ¿å’Œæ–‡ç« è³‡è¨Š
                board_name = "æœªçŸ¥çœ‹æ¿"
                article_id = "æœªçŸ¥ID"
                
                board_match = re.search(r'/bbs/([^/]+)/', link)
                if board_match:
                    board_name = board_match.group(1)
                
                id_match = re.search(r'/M\.(\d+)\.A\.([A-Z0-9]+)\.html', link)
                if id_match:
                    article_id = id_match.group(1)
                    title = f"PTT {board_name} æ¿ - æ–‡ç« ID: {article_id}"
                else:
                    title = f"PTT {board_name} æ¿"
                
                results.append({
                    'title': title,
                    'url': link,
                    'source': 'DuckDuckGo',
                    'board': board_name,
                    'article_id': article_id
                })
                
                print(f"âœ“ [{i+1:2d}] {board_name} - {article_id}")
                
    except Exception as e:
        print(f"DuckDuckGo æœç´¢éŒ¯èª¤: {e}")
    
    return results

def search_via_bing(query, max_results=10):
    """ä½¿ç”¨ Bing æœç´¢"""
    results = []
    
    try:
        search_query = f"{query} site:ptt.cc"
        encoded_query = urllib.parse.quote(search_query)
        
        url = f"https://www.bing.com/search?q={encoded_query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        }
        
        session = requests.Session()
        session.trust_env = False
        
        response = session.get(url, headers=headers, timeout=15, proxies={})
        
        if response.status_code == 200:
            content = response.text
            
            # æ‰¾å‡ºPTTé€£çµ
            ptt_pattern = r'https?://(?:www\.)?ptt\.cc/bbs/[^/]+/M\.\d+\.A\.[A-Z0-9]+\.html'
            ptt_links = list(set(re.findall(ptt_pattern, content)))
            
            for i, link in enumerate(ptt_links[:max_results]):
                # æå–çœ‹æ¿å’Œæ–‡ç« è³‡è¨Š
                board_name = "æœªçŸ¥çœ‹æ¿"
                article_id = "æœªçŸ¥ID"
                
                board_match = re.search(r'/bbs/([^/]+)/', link)
                if board_match:
                    board_name = board_match.group(1)
                
                id_match = re.search(r'/M\.(\d+)\.A\.([A-Z0-9]+)\.html', link)
                if id_match:
                    article_id = id_match.group(1)
                    title = f"PTT {board_name} æ¿ - æ–‡ç« ID: {article_id}"
                else:
                    title = f"PTT {board_name} æ¿"
                
                results.append({
                    'title': title,
                    'url': link,
                    'source': 'Bing',
                    'board': board_name,
                    'article_id': article_id
                })
                
                print(f"âœ“ [{i+1:2d}] {board_name} - {article_id}")
                
    except Exception as e:
        print(f"Bing æœç´¢éŒ¯èª¤: {e}")
    
    return results

def search_via_startpage_original(query, max_results=10):
    """
    ä½¿ç”¨ Startpage æœå°‹ ptt.cc ç›¸é—œçµæœ
    æ”¹é€²ç‰ˆï¼šæ·»åŠ æ›´å¥½çš„é™åˆ¶å’ŒéŒ¯èª¤è™•ç†
    """
    results = []
    
    # é™åˆ¶æœ€å¤§çµæœæ•¸ï¼Œé¿å…è«‹æ±‚éå¤š
    if max_results > 20:
        print(f"âš ï¸  å°‡æœ€å¤§çµæœæ•¸å¾ {max_results} èª¿æ•´ç‚º 20ï¼ˆé¿å…è¢«å°é–ï¼‰")
        max_results = 20
    
    try:
        print(f"ğŸ” æœå°‹é—œéµå­—: {query} (æœ€å¤š {max_results} ç­†)")
        
        search_query = f"{query} site:ptt.cc"
        encoded_query = urllib.parse.quote(search_query)
        
        url = f"https://www.startpage.com/sp/search?query={encoded_query}"
        print(f"ğŸŒ è«‹æ±‚URL: {url}")
        
        # æª¢æŸ¥ç¶²è·¯é€£ç·š
        try:
            test_response = requests.get("https://www.google.com", timeout=5)
            print(f"âœ… ç¶²è·¯é€£ç·šæ­£å¸¸ (Googleå›æ‡‰: {test_response.status_code})")
        except Exception as e:
            print(f"âŒ ç¶²è·¯é€£ç·šæ¸¬è©¦å¤±æ•—: {e}")
            print("è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šæˆ–ä»£ç†è¨­å®š")
            return results
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # æ·»åŠ é‡è©¦æ©Ÿåˆ¶
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦é€£æ¥ Startpage...")
                
                # æ¸…é™¤å¯èƒ½çš„ä»£ç†è¨­å®š
                session = requests.Session()
                session.trust_env = False  # å¿½ç•¥ç’°å¢ƒè®Šæ•¸ä¸­çš„ä»£ç†è¨­å®š
                
                response = session.get(url, headers=headers, timeout=15, proxies={})
                
                if response.status_code == 200:
                    content = response.text
                    
                    # æª¢æŸ¥æ˜¯å¦è¢«å°é–æˆ–é™åˆ¶
                    if "blocked" in content.lower() or "captcha" in content.lower():
                        print("âš ï¸  å¯èƒ½é‡åˆ°åçˆ¬èŸ²æ©Ÿåˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦")
                        return results
                    
                    # æ‰¾å‡ºæ‰€æœ‰ PTT é€£çµ
                    ptt_pattern = r'https?://(?:www\.)?ptt\.cc/bbs/[^/]+/M\.\d+\.A\.[A-Z0-9]+\.html'
                    ptt_links_raw = re.findall(ptt_pattern, content)
                    
                    if not ptt_links_raw:
                        print(f"ğŸ“ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦ï¼šæœªæ‰¾åˆ°PTTé€£çµ")
                        if attempt < max_retries - 1:
                            print(f"â³ ç­‰å¾… {(attempt + 1) * 2} ç§’å¾Œé‡è©¦...")
                            time.sleep((attempt + 1) * 2)
                            continue
                        else:
                            print("âŒ å¤šæ¬¡å˜—è©¦å¾Œä»æœªæ‰¾åˆ°çµæœ")
                            return results
                    
                    # æ¸…ç†å’Œå»é‡é€£çµ
                    ptt_links = []
                    for link in ptt_links_raw:
                        clean_link = link.replace('\\', '').strip()
                        if (clean_link.startswith('http') and 
                            'ptt.cc/bbs/' in clean_link and 
                            clean_link.endswith('.html') and
                            clean_link not in [r['url'] for r in results]):
                            ptt_links.append(clean_link)
                    
                    # å»é‡
                    ptt_links = list(set(ptt_links))
                    
                    print(f"ğŸ“Š æ‰¾åˆ° {len(ptt_links)} å€‹ä¸é‡è¤‡é€£çµ")
                    
                    # è™•ç†çµæœï¼Œé™åˆ¶æ•¸é‡
                    processed_count = 0
                    for link in ptt_links:
                        if processed_count >= max_results:
                            break
                            
                        # å¾é€£çµä¸­æå–è³‡è¨Š
                        board_name = "æœªçŸ¥çœ‹æ¿"
                        article_id = "æœªçŸ¥ID"
                        
                        # æå–çœ‹æ¿åç¨±
                        board_match = re.search(r'/bbs/([^/]+)/', link)
                        if board_match:
                            board_name = board_match.group(1)
                        
                        # æå–æ–‡ç« ID
                        id_match = re.search(r'/M\.(\d+)\.A\.([A-Z0-9]+)\.html', link)
                        if id_match:
                            article_id = id_match.group(1)
                            title = f"PTT {board_name} æ¿ - æ–‡ç« ID: {article_id}"
                        else:
                            title = f"PTT {board_name} æ¿"
                        
                        results.append({
                            'title': title,
                            'url': link,
                            'source': 'Startpage',
                            'board': board_name,
                            'article_id': article_id
                        })
                        
                        processed_count += 1
                        print(f"âœ“ [{processed_count:2d}] {board_name} - {article_id}")
                        
                        # çŸ­æš«å»¶é²ï¼Œé¿å…è«‹æ±‚éå¿«
                        time.sleep(0.3)
                    
                    break  # æˆåŠŸæ‰¾åˆ°çµæœï¼Œè·³å‡ºé‡è©¦å¾ªç’°
                    
                elif response.status_code == 429:
                    print(f"âš ï¸  è«‹æ±‚éæ–¼é »ç¹ (429)ï¼Œç­‰å¾… {(attempt + 1) * 5} ç§’å¾Œé‡è©¦...")
                    time.sleep((attempt + 1) * 5)
                    
                else:
                    print(f"âŒ HTTPéŒ¯èª¤ {response.status_code}")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                    
            except requests.exceptions.Timeout:
                print(f"â° è«‹æ±‚è¶…æ™‚ï¼Œç¬¬ {attempt + 1} æ¬¡å˜—è©¦")
                if attempt < max_retries - 1:
                    time.sleep(3)
                    
            except requests.exceptions.RequestException as e:
                print(f"ğŸŒ ç¶²è·¯éŒ¯èª¤: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    
    except Exception as e:
        print(f"âŒ æœå°‹éŒ¯èª¤: {e}")
    
    print(f"ğŸ¯ å®Œæˆæœå°‹ï¼Œå…±æ‰¾åˆ° {len(results)} å€‹çµæœ")
    return results


if __name__ == "__main__":
    print("PTT æœç´¢å·¥å…·")
    print("="*40)
    print("1. å–®æ¬¡æœç´¢")
    print("2. æ‰¹é‡æœç´¢ï¼ˆå¾æª”æ¡ˆè®€å–é—œéµå­—ï¼‰")
    print("="*40)
    
    mode = input("è«‹é¸æ“‡æ¨¡å¼ (1/2)ï¼š").strip()
    
    if mode == "2":
        # æ‰¹é‡æœç´¢æ¨¡å¼
        keywords_file = input("è«‹è¼¸å…¥é—œéµå­—æª”æ¡ˆåç¨±ï¼ˆé è¨­ï¼škeywords.txtï¼‰ï¼š").strip()
        if not keywords_file:
            keywords_file = "keywords.txt"
        
        max_results = input("æ¯å€‹é—œéµå­—è¦æŠ“å¹¾ç­†çµæœï¼ˆé è¨­ 10ï¼‰ï¼š").strip()
        try:
            max_results = int(max_results) if max_results else 10
        except ValueError:
            max_results = 10
        
        delay = input("æœç´¢é–“éš”ç§’æ•¸ï¼ˆé è¨­ 5 ç§’ï¼Œå»ºè­°ä¸å°‘æ–¼3ç§’ï¼‰ï¼š").strip()
        try:
            delay = int(delay) if delay else 5
            if delay < 3:
                print("âš ï¸  é–“éš”æ™‚é–“éçŸ­å¯èƒ½å°è‡´è¢«å°é–ï¼Œå»ºè­°è‡³å°‘3ç§’")
                delay = max(delay, 3)
        except ValueError:
            delay = 5
        
        # æª¢æŸ¥é—œéµå­—æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºç¯„ä¾‹æª”æ¡ˆ
        if not os.path.exists(keywords_file):
            print(f"\né—œéµå­—æª”æ¡ˆ {keywords_file} ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»ºç¯„ä¾‹æª”æ¡ˆ...")
            with open(keywords_file, 'w', encoding='utf-8') as f:
                f.write("# PTT æœç´¢é—œéµå­—åˆ—è¡¨\n")
                f.write("# æ¯è¡Œä¸€å€‹é—œéµå­—ï¼Œä»¥ # é–‹é ­çš„è¡Œç‚ºè¨»è§£\n")
                f.write("# è«‹ç·¨è¼¯æ­¤æª”æ¡ˆï¼Œæ·»åŠ æ‚¨æƒ³æœç´¢çš„é—œéµå­—\n\n")
                f.write("Python\n")
                f.write("æ©Ÿå™¨å­¸ç¿’\n")
                f.write("è‚¡ç¥¨\n")
                f.write("ç¾é£Ÿ\n")
            print(f"å·²å‰µå»ºç¯„ä¾‹æª”æ¡ˆ {keywords_file}")
            print("è«‹ç·¨è¼¯æ­¤æª”æ¡ˆï¼Œæ·»åŠ æ‚¨æƒ³æœç´¢çš„é—œéµå­—ï¼Œç„¶å¾Œé‡æ–°åŸ·è¡Œç¨‹å¼")
        else:
            # åŸ·è¡Œæ‰¹é‡æœç´¢
            batch_search_from_file(keywords_file, max_results, delay)
        
    else:
        # å–®æ¬¡æœç´¢æ¨¡å¼ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰
        keyword = input("è«‹è¼¸å…¥æƒ³æœå°‹çš„é—œéµå­—ï¼š")
        max_results = input("è¦æŠ“å¹¾ç­†çµæœï¼ˆé è¨­ 10ï¼‰ï¼š")
        
        try:
            max_results = int(max_results) if max_results else 10
        except ValueError:
            max_results = 10
        
        data = search_via_multiple_engines(keyword, max_results)
        
        # æ ¼å¼åŒ–è¼¸å‡ºçµæœ
        print("\n" + "="*60)
        print(f"æœå°‹é—œéµå­—ï¼š{keyword}")
        print(f"æ‰¾åˆ° {len(data)} å€‹ PTT ç›¸é—œé€£çµ")
        print("="*60)
        
        if data:
            for i, result in enumerate(data, 1):
                print(f"{i}. {result['title']}")
                print(f"   é€£çµï¼š{result['url']}")
                print(f"   ä¾†æºï¼š{result['source']}")
                print()
            
            # ä¿å­˜çµæœåˆ°æª”æ¡ˆ
            save_individual_results(data, keyword)
            
            # ç´¯ç©ä¿å­˜åˆ°ä¸»è¦ JSON æª”æ¡ˆï¼ˆå»é‡åŠŸèƒ½ï¼‰
            master_json_filename = "ptt_links_master.json"
            save_to_master_json(data, keyword, master_json_filename)
            
            print(f"çµæœå·²ä¿å­˜åˆ°ï¼š")
            print(f"  è©³ç´°çµæœï¼šptt_search_{keyword.replace(' ', '_').replace('/', '_')}.txt")
            print(f"  ç´¯ç©JSONï¼š{master_json_filename}")
            print(f"  ç´”é€£çµï¼šptt_urls_{keyword.replace(' ', '_').replace('/', '_')}.txt")
        else:
            print("æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„ PTT é€£çµ")
            print("å»ºè­°ï¼š")
            print("1. å˜—è©¦ä¸åŒçš„é—œéµå­—")
            print("2. ç›´æ¥åˆ° PTT ç¶²ç«™æœå°‹")
            print(f"3. æ‰‹å‹•æœå°‹ï¼šhttps://www.ptt.cc/bbs/search?q={urllib.parse.quote(keyword)}")
